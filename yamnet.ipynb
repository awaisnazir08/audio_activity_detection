{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import csv\n",
    "import io\n",
    "import json\n",
    "import librosa\n",
    "from sklearn.model_selection import train_test_split\n",
    "import librosa.display\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model.\n",
    "model = hub.load('https://www.kaggle.com/models/google/yamnet/TensorFlow2/yamnet/1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(267606,)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "waveform, _ = librosa.load('Audios/vid_5.mp3', sr=16000)\n",
    "waveform.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model, check the output.\n",
    "scores, embeddings, log_mel_spectrogram = model(waveform)\n",
    "scores.shape.assert_is_compatible_with([None, 521])\n",
    "embeddings.shape.assert_is_compatible_with([None, 1024])\n",
    "log_mel_spectrogram.shape.assert_is_compatible_with([None, 64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the name of the class with the top score when mean-aggregated across frames.\n",
    "def class_names_from_csv(class_map_csv_text):\n",
    "  \"\"\"Returns list of class names corresponding to score vector.\"\"\"\n",
    "  class_map_csv = io.StringIO(class_map_csv_text)\n",
    "  class_names = [display_name for (class_index, mid, display_name) in csv.reader(class_map_csv)]\n",
    "  class_names = class_names[1:]  # Skip CSV header\n",
    "  return class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speech\n"
     ]
    }
   ],
   "source": [
    "class_map_path = model.class_map_path().numpy()\n",
    "class_names = class_names_from_csv(tf.io.read_file(class_map_path).numpy().decode('utf-8'))\n",
    "print(class_names[scores.numpy().mean(axis=0).argmax()])  # Should print 'Silence'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([34, 1024])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, data_path = None, data = None, segment_len = 1000):\n",
    "        \"\"\"\n",
    "        Initialize the dataset with preprocessed data.\n",
    "        \n",
    "        Args:\n",
    "            data_path (str): Path to the preprocessed `.npz` file containing Mel spectrograms and labels.\n",
    "            segment_len (int): Fixed length of segments (in time steps) to extract for training.\n",
    "        \"\"\"\n",
    "        # data = np.load(data_path)\n",
    "        self.mel_specs = data['mel_specs']  # Shape: (n_mels, total_time_steps)\n",
    "        self.labels = data['labels']  # Shape: (total_time_steps,)\n",
    "        \n",
    "        self.segment_len = segment_len\n",
    "\n",
    "        # Total number of segments that can be extracted from the dataset\n",
    "        self.num_segments = self.mel_specs.shape[1] // self.segment_len\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the total number of segments in the dataset.\n",
    "        \"\"\"\n",
    "        return self.num_segments\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        \"\"\"\n",
    "        Retrieve a fixed-length segment of Mel spectrogram and corresponding labels.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the segment to retrieve.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (Mel spectrogram segment, Label segment)\n",
    "        \"\"\"\n",
    "        start = idx * self.segment_len\n",
    "        end = start + self.segment_len\n",
    "\n",
    "        mel_segment = self.mel_specs[:, start:min(end, self.mel_specs.shape[1])]\n",
    "        label_segment = self.labels[start:min(end, self.labels.shape[0])]\n",
    "\n",
    "        return torch.tensor(mel_segment, dtype=torch.float32), torch.tensor(label_segment, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNSoundClassifier(nn.Module):\n",
    "    def __init__(self, input_size, num_filters, segment_len=1000):\n",
    "        super(CNNSoundClassifier, self).__init__()\n",
    "        self.segment_len = segment_len\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(input_size, num_filters, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),  \n",
    "            nn.Dropout(p = 0.2),\n",
    "            \n",
    "            nn.Conv1d(num_filters, num_filters * 2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(p = 0.2)\n",
    "        )\n",
    "\n",
    "        reduced_dim = self.segment_len // (2 * 2) \n",
    "\n",
    "        self.fc = nn.Linear(reduced_dim * num_filters * 2, segment_len)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)  # Shape: (batch_size, num_filters * 2, reduced_dim)\n",
    "        x = x.view(x.size(0), -1)  # Flatten for the linear layer\n",
    "        x = self.fc(x)  # Shape: (batch_size, segment_len)\n",
    "        return self.sigmoid(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_timestamps(predictions, hop_length, sr, merge_threshold=2, duration_threshold=1):\n",
    "    \"\"\"\n",
    "    Extract timestamps from a binary prediction array, merging adjacent detections \n",
    "    and filtering out short detections based on duration.\n",
    "\n",
    "    Args:\n",
    "        predictions (list): Binary array of predictions (1 or 0), where 1 indicates the presence of an event.\n",
    "        hop_length (int): The hop length in samples (used to calculate time from frame indices).\n",
    "        sr (int): The sample rate (samples per second), used to convert indices to time.\n",
    "        merge_threshold (float, optional): The minimum time gap (in seconds) between adjacent detections to consider them separate. Default is 2 seconds.\n",
    "        duration_threshold (float, optional): The minimum duration (in seconds) of an event to keep. Default is 1 second.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tuples representing the start and end times of detected events (in seconds).\n",
    "    \"\"\"\n",
    "    timestamps = []\n",
    "    start = None\n",
    "\n",
    "    for i, pred in enumerate(predictions):\n",
    "        if pred > 0.5 and start is None:\n",
    "            start = i\n",
    "        elif pred <= 0.5 and start is not None:\n",
    "            end = i\n",
    "            # Convert frame indices to time and store the detected timestamp\n",
    "            timestamps.append((start * hop_length / sr, end * hop_length / sr))\n",
    "            start = None\n",
    "\n",
    "    # Handle the case where an event ends at the last frame\n",
    "    if start is not None:\n",
    "        timestamps.append((start * hop_length / sr, len(predictions) * hop_length / sr))\n",
    "    \n",
    "    merged_timestamps = []\n",
    "    for ts in timestamps:\n",
    "        if not merged_timestamps or ts[0] - merged_timestamps[-1][1] > merge_threshold:\n",
    "            # No overlap, append as new event\n",
    "            merged_timestamps.append(ts)\n",
    "        else:\n",
    "            # Merge adjacent events into one\n",
    "            merged_timestamps[-1] = (merged_timestamps[-1][0], ts[1])\n",
    "    \n",
    "    # Filter out events that are too short based on the duration_threshold\n",
    "    merged_timestamps = [ts for ts in merged_timestamps if ts[1] - ts[0] >= duration_threshold]\n",
    "    \n",
    "    return merged_timestamps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess all audio files to generate YAMNet embeddings and corresponding labels\n",
    "def preprocess_all_audios(audio_files, annotations, sr=16000, hop_length=160, root_path='Audios', save_path='preprocessed_data.npz'):\n",
    "    all_embeddings = None  # Initialize as None for the first file\n",
    "    all_labels = None\n",
    "\n",
    "    for audio_file in audio_files:\n",
    "        audio_path = os.path.join(root_path, audio_file)\n",
    "        waveform, _ = librosa.load(audio_path, sr=sr)  # Load the audio file with specified sample rate\n",
    "        \n",
    "        # Extract embeddings using YAMNet\n",
    "        # YAMNet requires 16kHz sampling rate and the audio must be monophonic\n",
    "        \n",
    "        # Extract YAMNet embeddings and corresponding temporal data\n",
    "        scores, embeddings, log_mel_spectrogram = model(waveform)  # Output embeddings and the corresponding time stamps\n",
    "        embeddings = embeddings.numpy()  # Convert embeddings to numpy array\n",
    "        \n",
    "        # Generate labels for each time frame\n",
    "        labels = np.zeros(embeddings.shape[0])  # Initialize labels for the time steps\n",
    "        timestamps = annotations[audio_file]['crowd_noise']  # Retrieve crowd noise timestamps\n",
    "        \n",
    "        for start, end in timestamps:\n",
    "            # Convert start and end times to indices based on hop_length and sample rate\n",
    "            start_idx = int(start * sr / hop_length)\n",
    "            end_idx = int(end * sr / hop_length)\n",
    "            \n",
    "            # Assign a label of 1 for the frames during the crowd noise event\n",
    "            labels[start_idx:end_idx] = 1\n",
    "\n",
    "        # Concatenate embeddings and labels\n",
    "        if all_embeddings is None:\n",
    "            all_embeddings = embeddings\n",
    "        else:\n",
    "            all_embeddings = np.concatenate((all_embeddings, embeddings), axis=0)\n",
    "\n",
    "        if all_labels is None:\n",
    "            all_labels = labels\n",
    "        else:\n",
    "            all_labels = np.concatenate((all_labels, labels))\n",
    "\n",
    "    # Save the final arrays\n",
    "    np.savez_compressed(save_path, embeddings=all_embeddings, labels=all_labels)\n",
    "    print(f\"Saved preprocessed data: Embeddings shape {all_embeddings.shape}, Labels length {all_labels.shape}\")\n",
    "    \n",
    "    return {'embeddings': all_embeddings, 'labels': all_labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=10):\n",
    "    \"\"\"\n",
    "    Train and validate the model for the specified number of epochs.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The neural network model to train.\n",
    "        train_loader (DataLoader): DataLoader for the training dataset.\n",
    "        val_loader (DataLoader): DataLoader for the validation dataset.\n",
    "        criterion (nn.Module): The loss function used for training.\n",
    "        optimizer (torch.optim.Optimizer): Optimizer for updating model weights.\n",
    "        epochs (int, optional): Number of training epochs. Defaults to 10.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_train_loss = 0\n",
    "        train_correct_predictions = 0\n",
    "        train_total_samples = 0\n",
    "\n",
    "        for mel_spec, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(mel_spec)\n",
    "            loss = criterion(outputs.squeeze(), labels.squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step() \n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            predictions = (outputs.squeeze() > 0.5).float()\n",
    "            train_correct_predictions += (predictions == labels).sum().item()\n",
    "            train_total_samples += labels.numel()\n",
    "\n",
    "        # Calculate average training loss and accuracy for this epoch\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_accuracy = train_correct_predictions / train_total_samples\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        total_val_loss = 0\n",
    "        val_correct_predictions = 0\n",
    "        val_total_samples = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for mel_spec, labels in val_loader:\n",
    "                outputs = model(mel_spec)\n",
    "\n",
    "                loss = criterion(outputs.squeeze(), labels.squeeze())\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "                predictions = (outputs.squeeze() > 0.5).float()\n",
    "                val_correct_predictions += (predictions == labels).sum().item()\n",
    "                val_total_samples += labels.numel()\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        val_accuracy = val_correct_predictions / val_total_samples\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} -> \"\n",
    "            f\"Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.4%}, \"\n",
    "            f\"Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.4%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hop Length: Number of samples between consecutive frames in the spectrogram\n",
    "HOP_LENGTH = 64\n",
    "# Number of Mel frequency bands for Mel spectrogram\n",
    "N_MELS = 64\n",
    "# Sample rate of the audio (samples per second)\n",
    "SAMPLE_RATE = 22050\n",
    "# Hidden size of the LSTM layer (number of LSTM units)\n",
    "# HIDDEN_SIZE = 64\n",
    "# Maximum length of the input sequence (number of frames)\n",
    "SEGMENT_LEN = 1000\n",
    "# Training number of epochs\n",
    "EPOCHS = 10\n",
    "# Audios root directory path\n",
    "DATASET_ROOT_DIRECTORY = 'Audios'\n",
    "\n",
    "LEARNING_RATE = 2e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load annotations from JSON file\n",
    "with open(\"timestamp_annotations.json\", \"r\") as f:\n",
    "    annotations = json.load(f)\n",
    "\n",
    "# Extract the audio files from the annotations\n",
    "audio_files = list(annotations.keys())\n",
    "\n",
    "# Split the dataset into training and validation sets (80% training, 20% validation)\n",
    "train_files, val_files = train_test_split(audio_files, test_size=0.2, random_state=42)\n",
    "\n",
    "train_data = preprocess_all_audios(train_files, annotations, sr=SAMPLE_RATE, n_mels=N_MELS, hop_length=HOP_LENGTH, root_path=DATASET_ROOT_DIRECTORY)\n",
    "\n",
    "val_data = preprocess_all_audios(val_files, annotations, sr=SAMPLE_RATE, n_mels=N_MELS, hop_length=HOP_LENGTH, root_path=DATASET_ROOT_DIRECTORY)\n",
    "\n",
    "# Initialize the datasets\n",
    "train_dataset = AudioDataset(\n",
    "    data = train_data,\n",
    "    segment_len=SEGMENT_LEN\n",
    ")\n",
    "\n",
    "val_dataset = AudioDataset(\n",
    "    data = val_data,\n",
    "    segment_len=SEGMENT_LEN\n",
    ")\n",
    "\n",
    "# Create DataLoader instances for both training and validation datasets\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "# model = LSTMSoundClassifier(input_size=N_MELS, hidden_size=HIDDEN_SIZE, num_layers=1, output_size=1)\n",
    "model = CNNSoundClassifier(input_size=N_MELS, num_filters=32, segment_len=SEGMENT_LEN)\n",
    "criterion = nn.BCELoss()  # Binary Cross Entropy Loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model, train_loader, val_loader, criterion, optimizer, epochs=EPOCHS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

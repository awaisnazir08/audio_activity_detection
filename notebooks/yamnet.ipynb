{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\HP\\AppData\\Roaming\\Python\\Python311\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "import csv\n",
    "import io\n",
    "import json\n",
    "import librosa\n",
    "from sklearn.model_selection import train_test_split\n",
    "import librosa.display\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the name of the class with the top score when mean-aggregated across frames.\n",
    "def class_names_from_csv(class_map_csv_text):\n",
    "    \"\"\"Returns list of class names corresponding to score vector.\"\"\"\n",
    "    class_map_csv = io.StringIO(class_map_csv_text)\n",
    "    class_names = [display_name for (class_index, mid, display_name) in csv.reader(class_map_csv)]\n",
    "    class_names = class_names[1:]  # Skip CSV header\n",
    "    return class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\HP\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\HP\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\HP\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow_hub\\module_v2.py:126: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\HP\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow_hub\\module_v2.py:126: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the model.\n",
    "yamnet_model = hub.load('https://www.kaggle.com/models/google/yamnet/TensorFlow2/yamnet/1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(491520,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "waveform, _ = librosa.load('Audios/rally_fan_noise_09.mp3', sr=16000)\n",
    "waveform.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1: 16000 samples\n",
      "Speech\n",
      "Chunk 2: 16000 samples\n",
      "Speech\n",
      "Chunk 3: 16000 samples\n",
      "Speech\n",
      "Chunk 4: 16000 samples\n",
      "Speech\n",
      "Chunk 5: 16000 samples\n",
      "Speech\n",
      "Chunk 6: 16000 samples\n",
      "Speech\n",
      "Chunk 7: 16000 samples\n",
      "Speech\n",
      "Chunk 8: 16000 samples\n",
      "Speech\n",
      "Chunk 9: 16000 samples\n",
      "Speech\n",
      "Chunk 10: 16000 samples\n",
      "Speech\n",
      "Chunk 11: 16000 samples\n",
      "Speech\n",
      "Chunk 12: 16000 samples\n",
      "Speech\n",
      "Chunk 13: 16000 samples\n",
      "Speech\n",
      "Chunk 14: 16000 samples\n",
      "Speech\n",
      "Chunk 15: 16000 samples\n",
      "Speech\n",
      "Chunk 16: 16000 samples\n",
      "Speech\n",
      "Chunk 17: 16000 samples\n",
      "Speech\n",
      "Chunk 18: 16000 samples\n",
      "Speech\n",
      "Chunk 19: 16000 samples\n",
      "Speech\n",
      "Chunk 20: 16000 samples\n",
      "Speech\n",
      "Chunk 21: 16000 samples\n",
      "Speech\n",
      "Chunk 22: 16000 samples\n",
      "Speech\n",
      "Chunk 23: 16000 samples\n",
      "Speech\n",
      "Chunk 24: 16000 samples\n",
      "Vehicle\n",
      "Chunk 25: 16000 samples\n",
      "Vehicle\n",
      "Chunk 26: 16000 samples\n",
      "Speech\n",
      "Chunk 27: 16000 samples\n",
      "Speech\n",
      "Chunk 28: 16000 samples\n",
      "Speech\n",
      "Chunk 29: 16000 samples\n",
      "Speech\n",
      "Chunk 30: 16000 samples\n",
      "Speech\n",
      "Chunk 31: 11520 samples\n",
      "Speech\n"
     ]
    }
   ],
   "source": [
    "def load_audio_in_chunks(audio_path, chunk_duration=1.0, sample_rate=16000):\n",
    "    audio, sr = librosa.load(audio_path, sr=sample_rate)\n",
    "    chunk_length = int(chunk_duration * sr)  # Number of samples in each chunk\n",
    "    chunks = [\n",
    "        audio[i : i + chunk_length] for i in range(0, len(audio), chunk_length)\n",
    "    ]\n",
    "    return chunks, sr\n",
    "\n",
    "# Example usage\n",
    "audio_path = \"Audios/rally_fan_noise_09.mp3\"\n",
    "chunks, sr = load_audio_in_chunks(audio_path)\n",
    "\n",
    "# Process each chunk\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Chunk {i+1}: {len(chunk)} samples\")\n",
    "    scores, embeddings, log_mel_spectrogram = yamnet_model(chunk)\n",
    "    class_map_path = yamnet_model.class_map_path().numpy()\n",
    "    class_names = class_names_from_csv(tf.io.read_file(class_map_path).numpy().decode('utf-8'))\n",
    "    print(class_names[scores.numpy().mean(axis=0).argmax()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the model, check the output.\n",
    "scores, embeddings, log_mel_spectrogram = yamnet_model(waveform)\n",
    "scores.shape.assert_is_compatible_with([None, 521])\n",
    "embeddings.shape.assert_is_compatible_with([None, 1024])\n",
    "log_mel_spectrogram.shape.assert_is_compatible_with([None, 64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speech\n"
     ]
    }
   ],
   "source": [
    "class_map_path = yamnet_model.class_map_path().numpy()\n",
    "class_names = class_names_from_csv(tf.io.read_file(class_map_path).numpy().decode('utf-8'))\n",
    "print(class_names[scores.numpy().mean(axis=0).argmax()]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([63, 1024])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([3072, 64])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_mel_spectrogram.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, data_path=None, data=None):\n",
    "        \n",
    "        if data is None:\n",
    "            data = np.load(data_path)\n",
    "\n",
    "        self.embeddings = data['embeddings']\n",
    "        self.labels = data['labels']\n",
    "\n",
    "        assert len(self.embeddings) == len(self.labels), \"Embeddings and labels must have the same length.\"\n",
    "        \n",
    "        self.embeddings = torch.tensor(self.embeddings, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(self.labels, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.embeddings) \n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        return self.embeddings[idx], self.labels[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNSoundClassifier(nn.Module):\n",
    "    def __init__(self, input_size, num_filters, segment_len=1000):\n",
    "        super(CNNSoundClassifier, self).__init__()\n",
    "        self.segment_len = segment_len\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(input_size, num_filters, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),  \n",
    "            nn.Dropout(p = 0.2),\n",
    "            \n",
    "            nn.Conv1d(num_filters, num_filters * 2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(p = 0.2)\n",
    "        )\n",
    "\n",
    "        reduced_dim = self.segment_len // (2 * 2) \n",
    "\n",
    "        self.fc = nn.Linear(reduced_dim * num_filters * 2, segment_len)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)  # Shape: (batch_size, num_filters * 2, reduced_dim)\n",
    "        x = x.view(x.size(0), -1)  # Flatten for the linear layer\n",
    "        x = self.fc(x)  # Shape: (batch_size, segment_len)\n",
    "        return self.sigmoid(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_timestamps(predictions, hop_length, sr, merge_threshold=2, duration_threshold=1):\n",
    "    \"\"\"\n",
    "    Extract timestamps from a binary prediction array, merging adjacent detections \n",
    "    and filtering out short detections based on duration.\n",
    "\n",
    "    Args:\n",
    "        predictions (list): Binary array of predictions (1 or 0), where 1 indicates the presence of an event.\n",
    "        hop_length (int): The hop length in samples (used to calculate time from frame indices).\n",
    "        sr (int): The sample rate (samples per second), used to convert indices to time.\n",
    "        merge_threshold (float, optional): The minimum time gap (in seconds) between adjacent detections to consider them separate. Default is 2 seconds.\n",
    "        duration_threshold (float, optional): The minimum duration (in seconds) of an event to keep. Default is 1 second.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tuples representing the start and end times of detected events (in seconds).\n",
    "    \"\"\"\n",
    "    timestamps = []\n",
    "    start = None\n",
    "\n",
    "    for i, pred in enumerate(predictions):\n",
    "        if pred > 0.5 and start is None:\n",
    "            start = i\n",
    "        elif pred <= 0.5 and start is not None:\n",
    "            end = i\n",
    "            # Convert frame indices to time and store the detected timestamp\n",
    "            timestamps.append((start * hop_length / sr, end * hop_length / sr))\n",
    "            start = None\n",
    "\n",
    "    # Handle the case where an event ends at the last frame\n",
    "    if start is not None:\n",
    "        timestamps.append((start * hop_length / sr, len(predictions) * hop_length / sr))\n",
    "    \n",
    "    merged_timestamps = []\n",
    "    for ts in timestamps:\n",
    "        if not merged_timestamps or ts[0] - merged_timestamps[-1][1] > merge_threshold:\n",
    "            # No overlap, append as new event\n",
    "            merged_timestamps.append(ts)\n",
    "        else:\n",
    "            # Merge adjacent events into one\n",
    "            merged_timestamps[-1] = (merged_timestamps[-1][0], ts[1])\n",
    "    \n",
    "    # Filter out events that are too short based on the duration_threshold\n",
    "    merged_timestamps = [ts for ts in merged_timestamps if ts[1] - ts[0] >= duration_threshold]\n",
    "    \n",
    "    return merged_timestamps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_all_audios(audio_files, annotations, sr=16000, root_path='Audios', save_path='preprocessed_data.npz'):\n",
    "#     all_embeddings = None \n",
    "#     all_labels = None\n",
    "\n",
    "#     for audio_file in audio_files:\n",
    "#         audio_path = os.path.join(root_path, audio_file)\n",
    "#         waveform, _ = librosa.load(audio_path, sr=sr)\n",
    "\n",
    "#         scores, embeddings, log_mel_spectrogram = yamnet_model(waveform)\n",
    "#         embeddings = embeddings.numpy()\n",
    "\n",
    "#         frame_length = 0.96  # seconds\n",
    "#         frame_hop = 0.48  # seconds\n",
    "\n",
    "#         num_embeddings = embeddings.shape[0]\n",
    "#         labels = np.zeros(num_embeddings)\n",
    "        \n",
    "#         timestamps = annotations[audio_file]['crowd_noise']\n",
    "\n",
    "#         for i in range(num_embeddings):\n",
    "#             # Calculate start and end times for the current embedding\n",
    "#             start_time = i * frame_hop\n",
    "#             end_time = start_time + frame_length\n",
    "\n",
    "#             # Check overlap with each crowd noise interval\n",
    "#             for start, end in timestamps:\n",
    "#                 overlap_start = max(start, start_time)\n",
    "#                 overlap_end = min(end, end_time)\n",
    "#                 overlap_duration = max(0, overlap_end - overlap_start)\n",
    "\n",
    "#                 # Label as 1 if overlap exceeds half the frame length\n",
    "#                 if overlap_duration >= (frame_length / 2):\n",
    "#                     labels[i] = 1\n",
    "#                     break  # No need to check further if labeled as 1\n",
    "\n",
    "#         # Concatenate embeddings and labels across all files\n",
    "#         if all_embeddings is None:\n",
    "#             all_embeddings = embeddings\n",
    "#         else:\n",
    "#             all_embeddings = np.concatenate((all_embeddings, embeddings), axis=0)\n",
    "\n",
    "#         if all_labels is None:\n",
    "#             all_labels = labels\n",
    "#         else:\n",
    "#             all_labels = np.concatenate((all_labels, labels))\n",
    "    \n",
    "#     # Save the preprocessed data\n",
    "#     # np.savez_compressed(save_path, embeddings=all_embeddings, labels=all_labels)\n",
    "#     print(f\"Saved preprocessed data: Embeddings shape {all_embeddings.shape}, Labels length {all_labels.shape}\")\n",
    "    \n",
    "#     return {'embeddings': all_embeddings, 'labels': all_labels}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing audio for 4 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_all_audios(audio_files, annotations, sr=16000, root_path='Audios', save_path='preprocessed_data.npz'):\n",
    "    all_embeddings = None \n",
    "    all_labels = None\n",
    "\n",
    "    # Define class names\n",
    "    class_names = ['crowd_noise', 'long_noise', 'tyre_screech_noise', 'crash_noise']\n",
    "                # [1, 2, 3, 4]\n",
    "    num_classes = len(class_names)\n",
    "\n",
    "    for audio_file in audio_files:\n",
    "        audio_path = os.path.join(root_path, audio_file)\n",
    "        waveform, _ = librosa.load(audio_path, sr=sr)\n",
    "\n",
    "        # Extract YAMNet embeddings\n",
    "        scores, embeddings, log_mel_spectrogram = yamnet_model(waveform)\n",
    "        embeddings = embeddings.numpy()\n",
    "\n",
    "        frame_length = 0.96  # seconds\n",
    "        frame_hop = 0.48  # seconds\n",
    "        num_embeddings = embeddings.shape[0]\n",
    "\n",
    "        # Create a (num_embeddings, num_classes) binary label matrix\n",
    "        labels = np.zeros((num_embeddings, num_classes))\n",
    "\n",
    "        # Iterate over each class and assign labels\n",
    "        for class_idx, class_name in enumerate(class_names):\n",
    "            if class_name not in annotations[audio_file]:\n",
    "                continue  # Skip if no annotations for this class\n",
    "\n",
    "            timestamps = annotations[audio_file][class_name]\n",
    "\n",
    "            for i in range(num_embeddings):\n",
    "                start_time = i * frame_hop\n",
    "                end_time = start_time + frame_length\n",
    "\n",
    "                # Check overlap with each annotated interval\n",
    "                for start, end in timestamps:\n",
    "                    overlap_start = max(start, start_time)\n",
    "                    overlap_end = min(end, end_time)\n",
    "                    overlap_duration = max(0, overlap_end - overlap_start)\n",
    "\n",
    "                    # Assign label if overlap exceeds half the frame length\n",
    "                    if overlap_duration >= (frame_length / 2):\n",
    "                        labels[i, class_idx] = 1\n",
    "                        break  # Stop checking further intervals\n",
    "\n",
    "        # Concatenate embeddings and labels across all files\n",
    "        if all_embeddings is None:\n",
    "            all_embeddings = embeddings\n",
    "        else:\n",
    "            all_embeddings = np.concatenate((all_embeddings, embeddings), axis=0)\n",
    "\n",
    "        if all_labels is None:\n",
    "            all_labels = labels\n",
    "        else:\n",
    "            all_labels = np.concatenate((all_labels, labels), axis=0)\n",
    "\n",
    "    # Save the preprocessed data\n",
    "    np.savez_compressed(save_path, embeddings=all_embeddings, labels=all_labels)\n",
    "    # print(f\"Saved preprocessed data: Embeddings shape {all_embeddings.shape}, Labels shape {all_labels.shape}\")\n",
    "    \n",
    "    return {'embeddings': all_embeddings, 'labels': all_labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the classifier head\n",
    "class ClassifierHead(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes = 4, hidden_dim=128):\n",
    "        \"\"\"\n",
    "        A simple classifier head to predict binary labels from embeddings.\n",
    "\n",
    "        Args:\n",
    "            input_dim (int): Dimensionality of the input embeddings.\n",
    "            hidden_dim (int): Number of units in the hidden layer.\n",
    "        \"\"\"\n",
    "        super(ClassifierHead, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(hidden_dim, num_classes) \n",
    "        # self.sigmoid = nn.Sigmoid() \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        # x = self.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample rate of the audio (samples per second)\n",
    "SAMPLE_RATE = 16000\n",
    "\n",
    "# Training number of epochs\n",
    "EPOCHS = 20\n",
    "\n",
    "# Audios root directory path\n",
    "DATASET_ROOT_DIRECTORY = 'Audios'\n",
    "\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "WEIGHT_DECAY = 1e-4  # Appropriate weight decay for small dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ClassifierHead(input_dim=1024, hidden_dim=128)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load annotations from JSON file\n",
    "with open(\"latest_timestamp_annotations.json\", \"r\") as f:\n",
    "    annotations = json.load(f)\n",
    "\n",
    "# Extract the audio files from the annotations\n",
    "audio_files = list(annotations.keys())\n",
    "\n",
    "# Split the dataset into training and validation sets (80% training, 20% validation)\n",
    "train_files, val_files = train_test_split(audio_files, test_size=0.2, random_state=21)\n",
    "\n",
    "train_data = preprocess_all_audios(train_files, annotations, root_path=DATASET_ROOT_DIRECTORY)\n",
    "\n",
    "val_data = preprocess_all_audios(val_files, annotations, sr=SAMPLE_RATE, root_path=DATASET_ROOT_DIRECTORY)\n",
    "\n",
    "# # Initialize the datasets\n",
    "train_dataset = AudioDataset(\n",
    "    data = train_data,\n",
    ")\n",
    "\n",
    "val_dataset = AudioDataset(\n",
    "    data = val_data,\n",
    ")\n",
    "\n",
    "# Create DataLoader instances for both training and validation datasets\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1408\n",
      "357\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader))\n",
    "print(len(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=10, threshold = 0.6):\n",
    "    \"\"\"\n",
    "    Train and validate the model for the specified number of epochs.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The neural network model to train.\n",
    "        train_loader (DataLoader): DataLoader for the training dataset.\n",
    "        val_loader (DataLoader): DataLoader for the validation dataset.\n",
    "        criterion (nn.Module): The loss function used for training.\n",
    "        optimizer (torch.optim.Optimizer): Optimizer for updating model weights.\n",
    "        epochs (int, optional): Number of training epochs. Defaults to 10.\n",
    "        device (str, optional): Device to train on. Defaults to 'cuda'.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    model.to(device)  # Move model to GPU if available\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        train_correct_samples = 0\n",
    "        train_total_samples = 0\n",
    "\n",
    "        for embeddings, labels in train_loader:\n",
    "            embeddings, labels = embeddings.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(embeddings)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step() \n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            # Multi-label accuracy (thresholding at 0.5)\n",
    "            predictions = (outputs > threshold).float()\n",
    "            train_correct_samples += (predictions == labels).all(dim=1).sum().item()  # Count only if all labels match\n",
    "            train_total_samples += labels.shape[0]  # Total number of samples\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        train_accuracy = train_correct_samples / train_total_samples  # Accuracy per sample\n",
    "\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        val_correct_samples = 0\n",
    "        val_total_samples = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for embeddings, labels in val_loader:\n",
    "                embeddings, labels = embeddings.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(embeddings)\n",
    "                loss = criterion(outputs, labels)\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "                # Multi-label accuracy (thresholding at 0.5)\n",
    "                predictions = (outputs > threshold).float()\n",
    "                val_correct_samples += (predictions == labels).all(dim=1).sum().item()\n",
    "                val_total_samples += labels.shape[0]\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        val_accuracy = val_correct_samples / val_total_samples  # Accuracy per sample\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} -> \"\n",
    "              f\"Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.4%}, \"\n",
    "              f\"Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.4%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 -> Train Loss: 0.1106, Train Accuracy: 86.4145%, Val Loss: 0.1549, Val Accuracy: 81.7927%\n",
      "Epoch 2/20 -> Train Loss: 0.1090, Train Accuracy: 86.4145%, Val Loss: 0.1561, Val Accuracy: 81.8627%\n",
      "Epoch 3/20 -> Train Loss: 0.1064, Train Accuracy: 86.9117%, Val Loss: 0.1635, Val Accuracy: 82.1429%\n",
      "Epoch 4/20 -> Train Loss: 0.1044, Train Accuracy: 86.9295%, Val Loss: 0.1519, Val Accuracy: 82.0028%\n",
      "Epoch 5/20 -> Train Loss: 0.1034, Train Accuracy: 87.3380%, Val Loss: 0.1665, Val Accuracy: 81.0924%\n",
      "Epoch 6/20 -> Train Loss: 0.1002, Train Accuracy: 87.2669%, Val Loss: 0.1615, Val Accuracy: 81.9328%\n",
      "Epoch 7/20 -> Train Loss: 0.1015, Train Accuracy: 87.6754%, Val Loss: 0.1521, Val Accuracy: 81.5826%\n",
      "Epoch 8/20 -> Train Loss: 0.0981, Train Accuracy: 87.5333%, Val Loss: 0.1695, Val Accuracy: 82.1429%\n",
      "Epoch 9/20 -> Train Loss: 0.0975, Train Accuracy: 87.6043%, Val Loss: 0.1596, Val Accuracy: 82.1429%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[42], line 33\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, train_loader, val_loader, criterion, optimizer, epochs, threshold)\u001b[0m\n\u001b[0;32m     31\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     32\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 33\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m     35\u001b[0m total_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Multi-label accuracy (thresholding at 0.5)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\optim\\optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    383\u001b[0m             )\n\u001b[1;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\optim\\adam.py:166\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    155\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    157\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    158\u001b[0m         group,\n\u001b[0;32m    159\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    163\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    164\u001b[0m         state_steps)\n\u001b[1;32m--> 166\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\optim\\adam.py:316\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    314\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 316\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\optim\\adam.py:391\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    388\u001b[0m     param \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mview_as_real(param)\n\u001b[0;32m    390\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m--> 391\u001b[0m \u001b[43mexp_avg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlerp_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    392\u001b[0m exp_avg_sq\u001b[38;5;241m.\u001b[39mmul_(beta2)\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad\u001b[38;5;241m.\u001b[39mconj(), value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[0;32m    394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_model(model, train_loader, val_loader, criterion, optimizer, epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = preprocess_all_audios(['vid_741.mp3'], annotations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform, _ = librosa.load('Audios/vid_741.mp3', sr=16000)\n",
    "\n",
    "# Pass the waveform through YAMNet to get embeddings\n",
    "with torch.no_grad():\n",
    "    scores, embeddings, _ = yamnet_model(waveform)  # Replace `model` with your YAMNet instance\n",
    "    embeddings = embeddings.numpy()  # Convert embeddings to NumPy array if needed\n",
    "\n",
    "# Convert embeddings to Torch tensor\n",
    "embeddings_tensor = torch.tensor(embeddings, dtype=torch.float32)\n",
    "\n",
    "# Put the classifier in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Get predictions\n",
    "with torch.no_grad():\n",
    "    predictions = model(embeddings_tensor)\n",
    "    predictions = torch.sigmoid(predictions).numpy()  # Apply sigmoid if using BCEWithLogitsLoss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.2555128e-04, 9.9859113e-01, 8.6152412e-02, 6.3162729e-06],\n",
       "       [5.4725807e-04, 9.9426454e-01, 4.9774341e-02, 1.0348833e-04],\n",
       "       [3.3055272e-04, 9.9865192e-01, 8.9200310e-02, 3.6009742e-04],\n",
       "       [3.8536920e-03, 9.8970360e-01, 3.9336178e-01, 1.8777044e-02],\n",
       "       [7.4463952e-03, 9.7771180e-01, 3.8656232e-01, 1.0256008e-01],\n",
       "       [7.6422824e-05, 9.9939656e-01, 4.4849187e-01, 8.9247350e-04],\n",
       "       [4.0198922e-02, 8.0246359e-01, 8.8748038e-02, 2.9243913e-01],\n",
       "       [7.4999496e-02, 2.9600933e-01, 1.2357598e-02, 3.9385554e-01],\n",
       "       [2.6649131e-02, 3.7512547e-01, 4.1470532e-03, 5.9371990e-01],\n",
       "       [2.8034974e-02, 4.4607252e-01, 1.4624724e-02, 2.1100454e-01],\n",
       "       [1.7509377e-02, 8.5920654e-02, 8.9934701e-04, 4.0765740e-02],\n",
       "       [7.1609575e-01, 8.7032475e-02, 1.8724872e-05, 2.9465836e-04],\n",
       "       [6.7455369e-01, 1.2088279e-01, 9.7728735e-05, 2.3984655e-03],\n",
       "       [1.5796210e-01, 1.6045561e-01, 1.1158787e-03, 2.8177034e-02],\n",
       "       [7.4564181e-02, 2.5050640e-02, 9.3791408e-05, 2.0504862e-02],\n",
       "       [9.1081637e-01, 6.4077958e-02, 1.5330373e-04, 4.1998941e-03]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0.],\n",
       "       [0., 1., 1., 0.],\n",
       "       [0., 1., 1., 0.],\n",
       "       [0., 1., 1., 0.],\n",
       "       [0., 1., 1., 0.],\n",
       "       [0., 1., 1., 0.],\n",
       "       [0., 1., 1., 0.],\n",
       "       [0., 1., 0., 1.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.316273e-06"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 1, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [0, 0, 0, 0],\n",
       "       [1, 0, 0, 0]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ['crowd_noise', 'loud_car_noise', 'tyre_screech_noise', 'crash_noise']\n",
    "# [1, 2, 3, 4]\n",
    "threshold = 0.5  # Adjust based on your model's calibration\n",
    "binary_predictions = (predictions > threshold).astype(int)\n",
    "binary_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(binary_predictions == result['labels']).all(axis=1).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected crowd_noise intervals:\n",
      "  Start: 2.88s, End: 4.32s\n",
      "Detected loud_car_noise intervals:\n",
      "  Start: 0.00s, End: 13.92s\n",
      "Detected tyre_screech_noise intervals:\n",
      "Detected crash_noise intervals:\n"
     ]
    }
   ],
   "source": [
    "hop_size = 0.48  # In seconds\n",
    "frame_length = 0.96  # Duration of each frame\n",
    "\n",
    "noise_labels = ['crowd_noise', 'loud_car_noise', 'tyre_screech_noise', 'crash_noise']\n",
    "timestamps = {0: [], 1: [], 2: [], 3: []}\n",
    "\n",
    "# Create timestamps from binary predictions\n",
    "for i, labels in enumerate(binary_predictions):\n",
    "    for idx, label in enumerate(labels):\n",
    "        if label == 1:  # If the noise is detected\n",
    "            start_time = i * hop_size\n",
    "            end_time = start_time + frame_length\n",
    "            timestamps[idx].append((start_time, end_time))\n",
    "\n",
    "# Function to merge overlapping or adjacent intervals\n",
    "def merge_intervals(intervals):\n",
    "    if not intervals:\n",
    "        return []\n",
    "    \n",
    "    # Sort intervals based on start times\n",
    "    intervals.sort()\n",
    "    \n",
    "    merged = [intervals[0]]\n",
    "    for current in intervals[1:]:\n",
    "        previous = merged[-1]\n",
    "        # Merge overlapping or adjacent intervals\n",
    "        if current[0] <= previous[1]:  \n",
    "            merged[-1] = (previous[0], max(previous[1], current[1]))\n",
    "        else:\n",
    "            merged.append(current)\n",
    "    \n",
    "    return merged\n",
    "\n",
    "# Apply merging for each type of noise\n",
    "merged_timestamps = {idx: merge_intervals(timestamps[idx]) for idx in timestamps}\n",
    "\n",
    "# Print results\n",
    "for idx, intervals in merged_timestamps.items():\n",
    "    print(f\"Detected {noise_labels[idx]} intervals:\")\n",
    "    for start, end in intervals:\n",
    "        print(f\"  Start: {start:.2f}s, End: {end:.2f}s\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved preprocessed data: Embeddings shape (23, 1024), Labels length (23,)\n"
     ]
    }
   ],
   "source": [
    "data = preprocess_all_audios(['vid_475.mp3'], annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = binary_predictions.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_predictions = new == data['labels']\n",
    "\n",
    "# Count the number of correct predictions (True values)\n",
    "correct_count = np.sum(correct_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_confusion_matrix_and_report(model, val_loader):\n",
    "    \"\"\"\n",
    "    Evaluates the model using the validation data loader and computes the confusion matrix and classification report.\n",
    "    \n",
    "    Args:\n",
    "        model (torch.nn.Module): The trained model.\n",
    "        val_loader (DataLoader): DataLoader containing validation data.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    all_preds = []  # List to store all predictions\n",
    "    all_labels = []  # List to store all true labels\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient computation during evaluation\n",
    "        for embeddings, labels in val_loader:\n",
    "\n",
    "            outputs = model(embeddings)\n",
    "            predictions = (outputs.squeeze() > 0.5).float()\n",
    "            \n",
    "            all_preds.extend(predictions.cpu().numpy().flatten())\n",
    "            all_labels.extend(labels.cpu().numpy().flatten())\n",
    "\n",
    "    # Calculate confusion matrix using true and predicted labels\n",
    "    cm = confusion_matrix(all_labels, all_preds, labels=[1, 0])  # 1 = Cheering, 0 = No Cheering\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"Cheering\", \"No Cheering\"])\n",
    "\n",
    "    # Print classification report (precision, recall, f1-score, etc.)\n",
    "    report = classification_report(all_labels, all_preds, target_names=[\"No Cheering\", \"Cheering\"])\n",
    "    print(report)\n",
    "\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_confusion_matrix_and_report(model, val_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
